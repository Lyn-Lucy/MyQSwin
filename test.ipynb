{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActQ: torch.Size([1, 3, 224, 224])\n",
      "Conv2dQ: torch.Size([1, 96, 56, 56])\n",
      "LayerNorm: torch.Size([1, 56, 56, 96])\n",
      "Q_PatchEmbed: torch.Size([1, 56, 56, 96])\n",
      "Identity: torch.Size([1, 56, 56, 96])\n",
      "LayerNorm: torch.Size([1, 56, 56, 96])\n",
      "ActQ: torch.Size([64, 49, 96])\n",
      "LinearQ: torch.Size([64, 49, 288])\n",
      "LayerNorm: torch.Size([64, 3, 49, 32])\n",
      "LayerNorm: torch.Size([64, 3, 49, 32])\n",
      "ActQ: torch.Size([64, 3, 49, 32])\n",
      "ActQ: torch.Size([64, 3, 49, 32])\n",
      "ActQ: torch.Size([64, 3, 49, 32])\n",
      "Dropout: torch.Size([64, 3, 49, 49])\n",
      "ActQ: torch.Size([64, 3, 49, 49])\n",
      "ActQ: torch.Size([64, 49, 96])\n",
      "LinearQ: torch.Size([64, 49, 96])\n",
      "Dropout: torch.Size([64, 49, 96])\n",
      "Q_WindowAttention: torch.Size([64, 49, 96])\n",
      "Q_WindowAttention: torch.Size([64, 3, 49, 49])\n",
      "Identity: torch.Size([1, 56, 56, 96])\n",
      "LayerNorm: torch.Size([1, 3136, 96])\n",
      "ActQ: torch.Size([1, 3136, 96])\n",
      "LinearQ: torch.Size([1, 3136, 384])\n",
      "GELU: torch.Size([1, 3136, 384])\n",
      "Dropout: torch.Size([1, 3136, 384])\n",
      "ActQ: torch.Size([1, 3136, 384])\n",
      "LinearQ: torch.Size([1, 3136, 96])\n",
      "Dropout: torch.Size([1, 3136, 96])\n",
      "Q_Mlp: torch.Size([1, 3136, 96])\n",
      "Identity: torch.Size([1, 3136, 96])\n",
      "Q_SwinTransformerBlock: torch.Size([1, 56, 56, 96])\n",
      "LayerNorm: torch.Size([1, 56, 56, 96])\n",
      "ActQ: torch.Size([64, 49, 96])\n",
      "LinearQ: torch.Size([64, 49, 288])\n",
      "LayerNorm: torch.Size([64, 3, 49, 32])\n",
      "LayerNorm: torch.Size([64, 3, 49, 32])\n",
      "ActQ: torch.Size([64, 3, 49, 32])\n",
      "ActQ: torch.Size([64, 3, 49, 32])\n",
      "ActQ: torch.Size([64, 3, 49, 32])\n",
      "Dropout: torch.Size([64, 3, 49, 49])\n",
      "ActQ: torch.Size([64, 3, 49, 49])\n",
      "ActQ: torch.Size([64, 49, 96])\n",
      "LinearQ: torch.Size([64, 49, 96])\n",
      "Dropout: torch.Size([64, 49, 96])\n",
      "Q_WindowAttention: torch.Size([64, 49, 96])\n",
      "Q_WindowAttention: torch.Size([64, 3, 49, 49])\n",
      "DropPath: torch.Size([1, 56, 56, 96])\n",
      "LayerNorm: torch.Size([1, 3136, 96])\n",
      "ActQ: torch.Size([1, 3136, 96])\n",
      "LinearQ: torch.Size([1, 3136, 384])\n",
      "GELU: torch.Size([1, 3136, 384])\n",
      "Dropout: torch.Size([1, 3136, 384])\n",
      "ActQ: torch.Size([1, 3136, 384])\n",
      "LinearQ: torch.Size([1, 3136, 96])\n",
      "Dropout: torch.Size([1, 3136, 96])\n",
      "Q_Mlp: torch.Size([1, 3136, 96])\n",
      "DropPath: torch.Size([1, 3136, 96])\n",
      "Q_SwinTransformerBlock: torch.Size([1, 56, 56, 96])\n",
      "Sequential: torch.Size([1, 56, 56, 96])\n",
      "Q_SwinTransformerStage: torch.Size([1, 56, 56, 96])\n",
      "LayerNorm: torch.Size([1, 28, 28, 384])\n",
      "ActQ: torch.Size([1, 28, 28, 384])\n",
      "LinearQ: torch.Size([1, 28, 28, 192])\n",
      "Q_PatchMerging: torch.Size([1, 28, 28, 192])\n",
      "LayerNorm: torch.Size([1, 28, 28, 192])\n",
      "ActQ: torch.Size([16, 49, 192])\n",
      "LinearQ: torch.Size([16, 49, 576])\n",
      "LayerNorm: torch.Size([16, 6, 49, 32])\n",
      "LayerNorm: torch.Size([16, 6, 49, 32])\n",
      "ActQ: torch.Size([16, 6, 49, 32])\n",
      "ActQ: torch.Size([16, 6, 49, 32])\n",
      "ActQ: torch.Size([16, 6, 49, 32])\n",
      "Dropout: torch.Size([16, 6, 49, 49])\n",
      "ActQ: torch.Size([16, 6, 49, 49])\n",
      "ActQ: torch.Size([16, 49, 192])\n",
      "LinearQ: torch.Size([16, 49, 192])\n",
      "Dropout: torch.Size([16, 49, 192])\n",
      "Q_WindowAttention: torch.Size([16, 49, 192])\n",
      "Q_WindowAttention: torch.Size([16, 6, 49, 49])\n",
      "DropPath: torch.Size([1, 28, 28, 192])\n",
      "LayerNorm: torch.Size([1, 784, 192])\n",
      "ActQ: torch.Size([1, 784, 192])\n",
      "LinearQ: torch.Size([1, 784, 768])\n",
      "GELU: torch.Size([1, 784, 768])\n",
      "Dropout: torch.Size([1, 784, 768])\n",
      "ActQ: torch.Size([1, 784, 768])\n",
      "LinearQ: torch.Size([1, 784, 192])\n",
      "Dropout: torch.Size([1, 784, 192])\n",
      "Q_Mlp: torch.Size([1, 784, 192])\n",
      "DropPath: torch.Size([1, 784, 192])\n",
      "Q_SwinTransformerBlock: torch.Size([1, 28, 28, 192])\n",
      "LayerNorm: torch.Size([1, 28, 28, 192])\n",
      "ActQ: torch.Size([16, 49, 192])\n",
      "LinearQ: torch.Size([16, 49, 576])\n",
      "LayerNorm: torch.Size([16, 6, 49, 32])\n",
      "LayerNorm: torch.Size([16, 6, 49, 32])\n",
      "ActQ: torch.Size([16, 6, 49, 32])\n",
      "ActQ: torch.Size([16, 6, 49, 32])\n",
      "ActQ: torch.Size([16, 6, 49, 32])\n",
      "Dropout: torch.Size([16, 6, 49, 49])\n",
      "ActQ: torch.Size([16, 6, 49, 49])\n",
      "ActQ: torch.Size([16, 49, 192])\n",
      "LinearQ: torch.Size([16, 49, 192])\n",
      "Dropout: torch.Size([16, 49, 192])\n",
      "Q_WindowAttention: torch.Size([16, 49, 192])\n",
      "Q_WindowAttention: torch.Size([16, 6, 49, 49])\n",
      "DropPath: torch.Size([1, 28, 28, 192])\n",
      "LayerNorm: torch.Size([1, 784, 192])\n",
      "ActQ: torch.Size([1, 784, 192])\n",
      "LinearQ: torch.Size([1, 784, 768])\n",
      "GELU: torch.Size([1, 784, 768])\n",
      "Dropout: torch.Size([1, 784, 768])\n",
      "ActQ: torch.Size([1, 784, 768])\n",
      "LinearQ: torch.Size([1, 784, 192])\n",
      "Dropout: torch.Size([1, 784, 192])\n",
      "Q_Mlp: torch.Size([1, 784, 192])\n",
      "DropPath: torch.Size([1, 784, 192])\n",
      "Q_SwinTransformerBlock: torch.Size([1, 28, 28, 192])\n",
      "Sequential: torch.Size([1, 28, 28, 192])\n",
      "Q_SwinTransformerStage: torch.Size([1, 28, 28, 192])\n",
      "LayerNorm: torch.Size([1, 14, 14, 768])\n",
      "ActQ: torch.Size([1, 14, 14, 768])\n",
      "LinearQ: torch.Size([1, 14, 14, 384])\n",
      "Q_PatchMerging: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 14, 14, 384])\n",
      "ActQ: torch.Size([4, 49, 384])\n",
      "LinearQ: torch.Size([4, 49, 1152])\n",
      "LayerNorm: torch.Size([4, 12, 49, 32])\n",
      "LayerNorm: torch.Size([4, 12, 49, 32])\n",
      "ActQ: torch.Size([4, 12, 49, 32])\n",
      "ActQ: torch.Size([4, 12, 49, 32])\n",
      "ActQ: torch.Size([4, 12, 49, 32])\n",
      "Dropout: torch.Size([4, 12, 49, 49])\n",
      "ActQ: torch.Size([4, 12, 49, 49])\n",
      "ActQ: torch.Size([4, 49, 384])\n",
      "LinearQ: torch.Size([4, 49, 384])\n",
      "Dropout: torch.Size([4, 49, 384])\n",
      "Q_WindowAttention: torch.Size([4, 49, 384])\n",
      "Q_WindowAttention: torch.Size([4, 12, 49, 49])\n",
      "DropPath: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 196, 384])\n",
      "ActQ: torch.Size([1, 196, 384])\n",
      "LinearQ: torch.Size([1, 196, 1536])\n",
      "GELU: torch.Size([1, 196, 1536])\n",
      "Dropout: torch.Size([1, 196, 1536])\n",
      "ActQ: torch.Size([1, 196, 1536])\n",
      "LinearQ: torch.Size([1, 196, 384])\n",
      "Dropout: torch.Size([1, 196, 384])\n",
      "Q_Mlp: torch.Size([1, 196, 384])\n",
      "DropPath: torch.Size([1, 196, 384])\n",
      "Q_SwinTransformerBlock: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 14, 14, 384])\n",
      "ActQ: torch.Size([4, 49, 384])\n",
      "LinearQ: torch.Size([4, 49, 1152])\n",
      "LayerNorm: torch.Size([4, 12, 49, 32])\n",
      "LayerNorm: torch.Size([4, 12, 49, 32])\n",
      "ActQ: torch.Size([4, 12, 49, 32])\n",
      "ActQ: torch.Size([4, 12, 49, 32])\n",
      "ActQ: torch.Size([4, 12, 49, 32])\n",
      "Dropout: torch.Size([4, 12, 49, 49])\n",
      "ActQ: torch.Size([4, 12, 49, 49])\n",
      "ActQ: torch.Size([4, 49, 384])\n",
      "LinearQ: torch.Size([4, 49, 384])\n",
      "Dropout: torch.Size([4, 49, 384])\n",
      "Q_WindowAttention: torch.Size([4, 49, 384])\n",
      "Q_WindowAttention: torch.Size([4, 12, 49, 49])\n",
      "DropPath: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 196, 384])\n",
      "ActQ: torch.Size([1, 196, 384])\n",
      "LinearQ: torch.Size([1, 196, 1536])\n",
      "GELU: torch.Size([1, 196, 1536])\n",
      "Dropout: torch.Size([1, 196, 1536])\n",
      "ActQ: torch.Size([1, 196, 1536])\n",
      "LinearQ: torch.Size([1, 196, 384])\n",
      "Dropout: torch.Size([1, 196, 384])\n",
      "Q_Mlp: torch.Size([1, 196, 384])\n",
      "DropPath: torch.Size([1, 196, 384])\n",
      "Q_SwinTransformerBlock: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 14, 14, 384])\n",
      "ActQ: torch.Size([4, 49, 384])\n",
      "LinearQ: torch.Size([4, 49, 1152])\n",
      "LayerNorm: torch.Size([4, 12, 49, 32])\n",
      "LayerNorm: torch.Size([4, 12, 49, 32])\n",
      "ActQ: torch.Size([4, 12, 49, 32])\n",
      "ActQ: torch.Size([4, 12, 49, 32])\n",
      "ActQ: torch.Size([4, 12, 49, 32])\n",
      "Dropout: torch.Size([4, 12, 49, 49])\n",
      "ActQ: torch.Size([4, 12, 49, 49])\n",
      "ActQ: torch.Size([4, 49, 384])\n",
      "LinearQ: torch.Size([4, 49, 384])\n",
      "Dropout: torch.Size([4, 49, 384])\n",
      "Q_WindowAttention: torch.Size([4, 49, 384])\n",
      "Q_WindowAttention: torch.Size([4, 12, 49, 49])\n",
      "DropPath: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 196, 384])\n",
      "ActQ: torch.Size([1, 196, 384])\n",
      "LinearQ: torch.Size([1, 196, 1536])\n",
      "GELU: torch.Size([1, 196, 1536])\n",
      "Dropout: torch.Size([1, 196, 1536])\n",
      "ActQ: torch.Size([1, 196, 1536])\n",
      "LinearQ: torch.Size([1, 196, 384])\n",
      "Dropout: torch.Size([1, 196, 384])\n",
      "Q_Mlp: torch.Size([1, 196, 384])\n",
      "DropPath: torch.Size([1, 196, 384])\n",
      "Q_SwinTransformerBlock: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 14, 14, 384])\n",
      "ActQ: torch.Size([4, 49, 384])\n",
      "LinearQ: torch.Size([4, 49, 1152])\n",
      "LayerNorm: torch.Size([4, 12, 49, 32])\n",
      "LayerNorm: torch.Size([4, 12, 49, 32])\n",
      "ActQ: torch.Size([4, 12, 49, 32])\n",
      "ActQ: torch.Size([4, 12, 49, 32])\n",
      "ActQ: torch.Size([4, 12, 49, 32])\n",
      "Dropout: torch.Size([4, 12, 49, 49])\n",
      "ActQ: torch.Size([4, 12, 49, 49])\n",
      "ActQ: torch.Size([4, 49, 384])\n",
      "LinearQ: torch.Size([4, 49, 384])\n",
      "Dropout: torch.Size([4, 49, 384])\n",
      "Q_WindowAttention: torch.Size([4, 49, 384])\n",
      "Q_WindowAttention: torch.Size([4, 12, 49, 49])\n",
      "DropPath: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 196, 384])\n",
      "ActQ: torch.Size([1, 196, 384])\n",
      "LinearQ: torch.Size([1, 196, 1536])\n",
      "GELU: torch.Size([1, 196, 1536])\n",
      "Dropout: torch.Size([1, 196, 1536])\n",
      "ActQ: torch.Size([1, 196, 1536])\n",
      "LinearQ: torch.Size([1, 196, 384])\n",
      "Dropout: torch.Size([1, 196, 384])\n",
      "Q_Mlp: torch.Size([1, 196, 384])\n",
      "DropPath: torch.Size([1, 196, 384])\n",
      "Q_SwinTransformerBlock: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 14, 14, 384])\n",
      "ActQ: torch.Size([4, 49, 384])\n",
      "LinearQ: torch.Size([4, 49, 1152])\n",
      "LayerNorm: torch.Size([4, 12, 49, 32])\n",
      "LayerNorm: torch.Size([4, 12, 49, 32])\n",
      "ActQ: torch.Size([4, 12, 49, 32])\n",
      "ActQ: torch.Size([4, 12, 49, 32])\n",
      "ActQ: torch.Size([4, 12, 49, 32])\n",
      "Dropout: torch.Size([4, 12, 49, 49])\n",
      "ActQ: torch.Size([4, 12, 49, 49])\n",
      "ActQ: torch.Size([4, 49, 384])\n",
      "LinearQ: torch.Size([4, 49, 384])\n",
      "Dropout: torch.Size([4, 49, 384])\n",
      "Q_WindowAttention: torch.Size([4, 49, 384])\n",
      "Q_WindowAttention: torch.Size([4, 12, 49, 49])\n",
      "DropPath: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 196, 384])\n",
      "ActQ: torch.Size([1, 196, 384])\n",
      "LinearQ: torch.Size([1, 196, 1536])\n",
      "GELU: torch.Size([1, 196, 1536])\n",
      "Dropout: torch.Size([1, 196, 1536])\n",
      "ActQ: torch.Size([1, 196, 1536])\n",
      "LinearQ: torch.Size([1, 196, 384])\n",
      "Dropout: torch.Size([1, 196, 384])\n",
      "Q_Mlp: torch.Size([1, 196, 384])\n",
      "DropPath: torch.Size([1, 196, 384])\n",
      "Q_SwinTransformerBlock: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 14, 14, 384])\n",
      "ActQ: torch.Size([4, 49, 384])\n",
      "LinearQ: torch.Size([4, 49, 1152])\n",
      "LayerNorm: torch.Size([4, 12, 49, 32])\n",
      "LayerNorm: torch.Size([4, 12, 49, 32])\n",
      "ActQ: torch.Size([4, 12, 49, 32])\n",
      "ActQ: torch.Size([4, 12, 49, 32])\n",
      "ActQ: torch.Size([4, 12, 49, 32])\n",
      "Dropout: torch.Size([4, 12, 49, 49])\n",
      "ActQ: torch.Size([4, 12, 49, 49])\n",
      "ActQ: torch.Size([4, 49, 384])\n",
      "LinearQ: torch.Size([4, 49, 384])\n",
      "Dropout: torch.Size([4, 49, 384])\n",
      "Q_WindowAttention: torch.Size([4, 49, 384])\n",
      "Q_WindowAttention: torch.Size([4, 12, 49, 49])\n",
      "DropPath: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 196, 384])\n",
      "ActQ: torch.Size([1, 196, 384])\n",
      "LinearQ: torch.Size([1, 196, 1536])\n",
      "GELU: torch.Size([1, 196, 1536])\n",
      "Dropout: torch.Size([1, 196, 1536])\n",
      "ActQ: torch.Size([1, 196, 1536])\n",
      "LinearQ: torch.Size([1, 196, 384])\n",
      "Dropout: torch.Size([1, 196, 384])\n",
      "Q_Mlp: torch.Size([1, 196, 384])\n",
      "DropPath: torch.Size([1, 196, 384])\n",
      "Q_SwinTransformerBlock: torch.Size([1, 14, 14, 384])\n",
      "Sequential: torch.Size([1, 14, 14, 384])\n",
      "Q_SwinTransformerStage: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 7, 7, 1536])\n",
      "ActQ: torch.Size([1, 7, 7, 1536])\n",
      "LinearQ: torch.Size([1, 7, 7, 768])\n",
      "Q_PatchMerging: torch.Size([1, 7, 7, 768])\n",
      "LayerNorm: torch.Size([1, 7, 7, 768])\n",
      "ActQ: torch.Size([1, 49, 768])\n",
      "LinearQ: torch.Size([1, 49, 2304])\n",
      "LayerNorm: torch.Size([1, 24, 49, 32])\n",
      "LayerNorm: torch.Size([1, 24, 49, 32])\n",
      "ActQ: torch.Size([1, 24, 49, 32])\n",
      "ActQ: torch.Size([1, 24, 49, 32])\n",
      "ActQ: torch.Size([1, 24, 49, 32])\n",
      "Dropout: torch.Size([1, 24, 49, 49])\n",
      "ActQ: torch.Size([1, 24, 49, 49])\n",
      "ActQ: torch.Size([1, 49, 768])\n",
      "LinearQ: torch.Size([1, 49, 768])\n",
      "Dropout: torch.Size([1, 49, 768])\n",
      "Q_WindowAttention: torch.Size([1, 49, 768])\n",
      "Q_WindowAttention: torch.Size([1, 24, 49, 49])\n",
      "DropPath: torch.Size([1, 7, 7, 768])\n",
      "LayerNorm: torch.Size([1, 49, 768])\n",
      "ActQ: torch.Size([1, 49, 768])\n",
      "LinearQ: torch.Size([1, 49, 3072])\n",
      "GELU: torch.Size([1, 49, 3072])\n",
      "Dropout: torch.Size([1, 49, 3072])\n",
      "ActQ: torch.Size([1, 49, 3072])\n",
      "LinearQ: torch.Size([1, 49, 768])\n",
      "Dropout: torch.Size([1, 49, 768])\n",
      "Q_Mlp: torch.Size([1, 49, 768])\n",
      "DropPath: torch.Size([1, 49, 768])\n",
      "Q_SwinTransformerBlock: torch.Size([1, 7, 7, 768])\n",
      "LayerNorm: torch.Size([1, 7, 7, 768])\n",
      "ActQ: torch.Size([1, 49, 768])\n",
      "LinearQ: torch.Size([1, 49, 2304])\n",
      "LayerNorm: torch.Size([1, 24, 49, 32])\n",
      "LayerNorm: torch.Size([1, 24, 49, 32])\n",
      "ActQ: torch.Size([1, 24, 49, 32])\n",
      "ActQ: torch.Size([1, 24, 49, 32])\n",
      "ActQ: torch.Size([1, 24, 49, 32])\n",
      "Dropout: torch.Size([1, 24, 49, 49])\n",
      "ActQ: torch.Size([1, 24, 49, 49])\n",
      "ActQ: torch.Size([1, 49, 768])\n",
      "LinearQ: torch.Size([1, 49, 768])\n",
      "Dropout: torch.Size([1, 49, 768])\n",
      "Q_WindowAttention: torch.Size([1, 49, 768])\n",
      "Q_WindowAttention: torch.Size([1, 24, 49, 49])\n",
      "DropPath: torch.Size([1, 7, 7, 768])\n",
      "LayerNorm: torch.Size([1, 49, 768])\n",
      "ActQ: torch.Size([1, 49, 768])\n",
      "LinearQ: torch.Size([1, 49, 3072])\n",
      "GELU: torch.Size([1, 49, 3072])\n",
      "Dropout: torch.Size([1, 49, 3072])\n",
      "ActQ: torch.Size([1, 49, 3072])\n",
      "LinearQ: torch.Size([1, 49, 768])\n",
      "Dropout: torch.Size([1, 49, 768])\n",
      "Q_Mlp: torch.Size([1, 49, 768])\n",
      "DropPath: torch.Size([1, 49, 768])\n",
      "Q_SwinTransformerBlock: torch.Size([1, 7, 7, 768])\n",
      "Sequential: torch.Size([1, 7, 7, 768])\n",
      "Q_SwinTransformerStage: torch.Size([1, 7, 7, 768])\n",
      "Sequential: torch.Size([1, 7, 7, 768])\n",
      "LayerNorm: torch.Size([1, 7, 7, 768])\n",
      "FastAdaptiveAvgPool: torch.Size([1, 768])\n",
      "Identity: torch.Size([1, 768])\n",
      "SelectAdaptivePool2d: torch.Size([1, 768])\n",
      "Dropout: torch.Size([1, 768])\n",
      "LinearQ: torch.Size([1, 1000])\n",
      "Identity: torch.Size([1, 1000])\n",
      "ClassifierHead: torch.Size([1, 1000])\n",
      "SwinTransformer: torch.Size([1, 1000])\n",
      "SwinTransformer(\n",
      "  (patch_embed): Q_PatchEmbed(\n",
      "    (proj): Conv2dQ(\n",
      "      3, 96, kernel_size=(4, 4), stride=(4, 4), {'nbits': 8, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "      (act): ActQ({'nbits': 8, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "    )\n",
      "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (layers): Sequential(\n",
      "    (0): Q_SwinTransformerStage(\n",
      "      (downsample): Identity()\n",
      "      (blocks): Sequential(\n",
      "        (0): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=96, out_features=288, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=96, out_features=96, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=96, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=384, out_features=96, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (1): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=96, out_features=288, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=96, out_features=96, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.009)\n",
      "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=96, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=384, out_features=96, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.009)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Q_SwinTransformerStage(\n",
      "      (downsample): Q_PatchMerging(\n",
      "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (reduction): LinearQ(\n",
      "          in_features=384, out_features=192, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "          (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "        )\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=192, out_features=576, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=192, out_features=192, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.018)\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=192, out_features=768, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=768, out_features=192, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.018)\n",
      "        )\n",
      "        (1): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=192, out_features=576, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=192, out_features=192, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.027)\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=192, out_features=768, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=768, out_features=192, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.027)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Q_SwinTransformerStage(\n",
      "      (downsample): Q_PatchMerging(\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (reduction): LinearQ(\n",
      "          in_features=768, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "          (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "        )\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=384, out_features=1152, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=384, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.036)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=384, out_features=1536, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=1536, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.036)\n",
      "        )\n",
      "        (1): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=384, out_features=1152, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=384, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.045)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=384, out_features=1536, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=1536, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.045)\n",
      "        )\n",
      "        (2): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=384, out_features=1152, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=384, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.055)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=384, out_features=1536, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=1536, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.055)\n",
      "        )\n",
      "        (3): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=384, out_features=1152, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=384, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.064)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=384, out_features=1536, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=1536, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.064)\n",
      "        )\n",
      "        (4): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=384, out_features=1152, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=384, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.073)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=384, out_features=1536, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=1536, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.073)\n",
      "        )\n",
      "        (5): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=384, out_features=1152, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=384, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.082)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=384, out_features=1536, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=1536, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.082)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Q_SwinTransformerStage(\n",
      "      (downsample): Q_PatchMerging(\n",
      "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        (reduction): LinearQ(\n",
      "          in_features=1536, out_features=768, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "          (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "        )\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=768, out_features=2304, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=768, out_features=768, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.091)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=768, out_features=3072, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=3072, out_features=768, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.091)\n",
      "        )\n",
      "        (1): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=768, out_features=2304, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=768, out_features=768, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.100)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=768, out_features=3072, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=3072, out_features=768, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): ClassifierHead(\n",
      "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (fc): LinearQ(\n",
      "      in_features=768, out_features=1000, bias=True, fake\n",
      "      (act): ActQ(fake)\n",
      "    )\n",
      "    (flatten): Identity()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3403e-01,  2.3857e-01,  1.9115e-01,  1.2674e-01, -3.3599e-02,\n",
       "          8.4137e-02, -1.2973e-01,  6.9838e-02, -2.4702e-01, -2.3309e-02,\n",
       "          2.2417e-02,  4.7711e-01,  2.8598e-01, -2.6353e-01,  4.8838e-02,\n",
       "          3.2526e-02, -2.3265e-01, -3.2302e-01, -1.7595e-01, -2.6442e-01,\n",
       "         -2.5347e-01, -2.8756e-01, -6.2567e-01, -2.6849e-01,  4.7452e-01,\n",
       "         -1.8368e-01,  5.2150e-02, -4.6135e-02,  6.5200e-02, -1.9983e-01,\n",
       "          5.9653e-02, -3.1321e-01, -5.3178e-02,  1.5881e-01,  3.7975e-01,\n",
       "          9.0337e-02,  1.8684e-01,  3.1932e-01, -2.8448e-02, -5.3590e-02,\n",
       "         -1.7992e-02,  1.5348e-01, -2.6809e-01,  1.7327e-01, -2.2342e-01,\n",
       "         -7.1954e-02, -2.4783e-01,  3.0519e-01,  2.9427e-01,  1.2082e-01,\n",
       "         -2.8312e-01,  2.0871e-01,  3.0494e-01,  1.7009e-02,  2.0573e-01,\n",
       "          1.6654e-01, -5.0305e-02, -1.0758e-01, -7.0687e-03, -1.6362e-01,\n",
       "         -3.4280e-01,  9.7805e-02, -3.4921e-01,  3.3272e-01,  1.3467e-01,\n",
       "         -1.7886e-01, -7.2464e-02, -2.3781e-01,  2.0928e-01,  7.0710e-02,\n",
       "         -3.8308e-01, -2.4697e-02,  4.8831e-01,  8.3684e-02, -1.0074e-01,\n",
       "          3.1541e-02, -1.5612e-01, -5.0350e-01, -2.3649e-01, -1.0194e-01,\n",
       "         -8.6564e-02, -1.4609e-01,  9.2127e-02, -7.8147e-02, -2.4097e-01,\n",
       "          1.3950e-01, -5.0949e-02,  1.4966e-01,  1.3375e-01,  1.2343e-01,\n",
       "         -3.0226e-01, -5.7219e-01, -7.9938e-02,  2.0951e-01, -1.1468e-01,\n",
       "          3.7831e-01,  1.5591e-01, -5.3462e-01, -2.6695e-01,  1.6320e-01,\n",
       "          6.5314e-02,  6.3520e-02,  1.9406e-01,  1.7777e-01,  3.0575e-01,\n",
       "          2.8161e-02,  1.1798e-01, -2.9232e-01,  3.4832e-01,  9.3720e-02,\n",
       "          1.9650e-01, -1.4217e-01, -4.7291e-02, -2.9800e-02, -3.3989e-01,\n",
       "         -3.1788e-01, -2.7845e-01, -3.1476e-01, -4.6185e-01,  4.6697e-01,\n",
       "         -6.2943e-02, -2.8293e-01, -2.5908e-02, -2.8437e-01,  8.2326e-02,\n",
       "         -1.2814e-01, -3.7292e-01, -6.3268e-02, -1.1307e-01,  2.6445e-01,\n",
       "         -1.8819e-02,  3.4676e-01, -3.2112e-01,  1.5334e-01,  1.6496e-01,\n",
       "         -1.5191e-01, -3.0011e-01,  2.8599e-01, -3.9944e-01,  1.0726e-01,\n",
       "          1.2640e-01,  4.1039e-01, -3.3393e-02,  1.4441e-01, -4.3053e-01,\n",
       "         -1.8604e-03,  1.0277e-01, -2.0266e-01, -8.4138e-02,  1.1744e-01,\n",
       "         -2.8978e-01,  9.4238e-02, -4.3805e-01, -6.7526e-02, -2.3790e-04,\n",
       "          1.8648e-01,  2.7081e-01, -1.7154e-01, -3.7696e-01,  2.5027e-01,\n",
       "          1.3857e-01, -1.2153e-01,  1.3778e-01,  2.0404e-01,  2.1840e-01,\n",
       "         -8.4945e-02, -7.2758e-02,  3.5873e-01, -5.1373e-02,  7.4165e-01,\n",
       "         -1.5770e-01, -1.4179e-01,  7.2637e-02, -1.6954e-01, -1.6460e-01,\n",
       "          2.4398e-01, -2.1128e-01, -1.7560e-02,  7.6836e-02,  4.1999e-02,\n",
       "          2.5591e-02,  5.3174e-01, -1.1170e-02, -5.9888e-02, -1.4519e-01,\n",
       "          2.6169e-01,  3.8328e-01,  3.7911e-01, -3.1277e-01,  8.1462e-02,\n",
       "         -4.1558e-01, -6.6760e-02,  3.1920e-01,  1.0722e-01, -4.1098e-02,\n",
       "         -3.8131e-03, -2.5722e-01, -4.0614e-01, -1.8429e-01,  9.4767e-02,\n",
       "          2.6020e-01,  2.8783e-01,  1.4955e-02,  5.1251e-02, -2.4388e-01,\n",
       "          1.6798e-01, -2.1778e-01, -5.8438e-04,  1.2634e-01, -1.9313e-01,\n",
       "         -2.9047e-01, -7.1771e-02, -4.5701e-02, -8.9543e-02,  2.9063e-01,\n",
       "         -2.0188e-01, -3.9242e-01, -3.1319e-01, -2.4017e-01,  1.5458e-01,\n",
       "          2.9478e-02,  2.5918e-02, -1.2791e-01, -2.4439e-01,  9.9808e-02,\n",
       "         -1.5347e-01, -4.3868e-01,  1.4550e-01, -3.4214e-01,  2.7350e-01,\n",
       "          1.3763e-01,  4.1501e-01,  1.7731e-01, -3.0266e-01, -9.6492e-02,\n",
       "          4.5739e-01, -4.8671e-01,  9.0663e-02,  4.6077e-01, -1.3618e-01,\n",
       "          3.1425e-01,  1.1929e-01,  2.5826e-01, -7.0974e-02, -5.1753e-02,\n",
       "          1.9983e-01,  3.0379e-01,  2.4056e-01, -3.5418e-01,  2.2907e-01,\n",
       "          8.8853e-02,  2.0608e-01, -2.1056e-01, -6.3703e-01, -3.4437e-02,\n",
       "          5.1970e-02,  1.3076e-02, -4.3225e-01,  9.4568e-04, -2.9111e-01,\n",
       "          7.6156e-02,  6.1743e-02, -3.2090e-01,  2.4753e-01,  2.4321e-02,\n",
       "         -1.3538e-01, -1.4605e-01, -1.4316e-01, -6.2920e-06,  3.1658e-01,\n",
       "          1.1650e-01, -7.3334e-02, -1.7537e-01, -2.2126e-02, -2.0916e-02,\n",
       "          5.6544e-02,  2.9900e-02, -1.2136e-01, -2.7876e-01, -1.7890e-01,\n",
       "          2.3974e-03,  7.2788e-02, -6.6318e-02, -2.3643e-01, -2.9499e-01,\n",
       "         -4.0046e-02,  2.2692e-01, -2.5504e-01, -4.9995e-02, -1.7289e-01,\n",
       "          2.0684e-01, -2.5162e-01,  3.0235e-01,  2.1222e-01,  4.5576e-01,\n",
       "         -1.9892e-01,  5.4980e-02, -2.4455e-02,  1.0903e-02, -9.5199e-02,\n",
       "         -9.8016e-02,  4.4443e-02,  9.6306e-02, -1.1230e-01, -1.0608e-02,\n",
       "         -2.9598e-01, -8.6859e-02,  1.3924e-01,  5.3958e-01,  4.8906e-01,\n",
       "         -2.2434e-02, -3.2289e-01, -5.3001e-01, -6.1659e-02, -5.8752e-01,\n",
       "         -1.2119e-01,  1.8012e-01, -6.6224e-02, -8.4574e-02, -5.2725e-01,\n",
       "         -2.3838e-02,  5.4027e-02, -1.2189e-01, -1.2531e-01,  1.1288e-01,\n",
       "         -3.8675e-02, -1.5293e-01, -1.1037e-03,  2.6189e-01, -6.8523e-03,\n",
       "         -2.1116e-01, -3.3465e-03, -1.1914e-01, -1.4141e-02,  1.6770e-01,\n",
       "         -6.4653e-04,  9.1804e-02, -1.6682e-01, -1.4747e-02, -1.1112e-01,\n",
       "          2.4061e-02,  1.7851e-01, -1.3395e-01,  1.6359e-01, -1.1528e-01,\n",
       "         -6.5611e-02,  4.1184e-02, -2.6713e-02,  3.1956e-01,  2.6380e-01,\n",
       "          1.1911e-01,  1.7822e-02, -9.4156e-02, -4.5085e-01, -1.3349e-02,\n",
       "          1.8482e-01,  2.0650e-01, -7.2841e-02, -8.1860e-02, -1.0368e-01,\n",
       "          8.7852e-02,  1.8380e-01, -2.5046e-01, -1.2420e-01,  1.6726e-01,\n",
       "          3.5135e-02,  1.6246e-01, -2.5567e-01, -1.9386e-01, -1.4036e-01,\n",
       "          4.0420e-01, -2.6946e-01, -1.1334e-02,  3.1392e-02, -7.8514e-02,\n",
       "          1.0943e-01,  2.4060e-01,  2.6395e-01,  7.0634e-02, -6.0402e-01,\n",
       "         -5.0076e-02, -9.9135e-02, -1.6651e-01, -6.0758e-02,  9.9106e-02,\n",
       "         -1.0307e-01, -1.3878e-01, -9.6608e-02,  1.1929e-01,  1.0836e-01,\n",
       "          3.0979e-01, -4.3460e-01,  2.6521e-01, -6.6067e-02,  1.1317e-01,\n",
       "          3.3361e-01,  4.0200e-02,  9.3937e-03, -1.6807e-01,  1.5822e-01,\n",
       "         -3.0893e-01,  2.9317e-02, -1.3981e-01, -1.8982e-02,  2.0435e-01,\n",
       "         -1.8141e-01,  1.3369e-01,  4.2673e-01,  2.1878e-02,  2.5483e-03,\n",
       "          1.1079e-02,  2.2409e-01,  4.1229e-03,  1.6640e-01,  1.1556e-01,\n",
       "         -1.4980e-01,  4.0599e-02, -2.3188e-01,  1.3441e-01,  3.9338e-02,\n",
       "          1.1522e-01, -2.1192e-01, -1.3753e-01, -1.0239e-01, -1.5197e-01,\n",
       "          8.8944e-02, -4.5297e-02, -3.2118e-01, -1.0327e-01, -2.5255e-01,\n",
       "         -1.3737e-02, -9.3323e-02, -3.1057e-01, -1.3333e-01, -1.5284e-01,\n",
       "          3.1045e-01, -5.4791e-03, -1.0457e-03, -2.1592e-01,  2.7585e-01,\n",
       "          1.4347e-01,  4.7298e-01,  5.3133e-01, -1.0005e-01, -4.4152e-01,\n",
       "         -1.2388e-01,  6.5533e-02,  2.1618e-01, -9.6623e-02, -7.4407e-02,\n",
       "          2.0076e-01, -8.1378e-02,  9.9690e-02,  3.5080e-01, -2.5432e-01,\n",
       "         -2.7661e-02,  2.4044e-01, -4.1503e-02, -1.4671e-01, -5.2534e-01,\n",
       "          1.6759e-01,  1.2148e-01,  7.4227e-02, -2.4851e-01,  1.4344e-01,\n",
       "         -2.0063e-01, -4.4543e-01,  1.2023e-01, -4.2260e-02, -5.3126e-02,\n",
       "         -4.6541e-02, -1.3872e-01,  3.8677e-01,  4.2359e-01,  6.6217e-01,\n",
       "         -1.8808e-01,  1.7972e-01,  4.7910e-01, -1.3977e-01,  1.7838e-01,\n",
       "          2.5611e-01, -2.4588e-01,  3.9757e-02, -9.6967e-02, -1.1168e-01,\n",
       "          1.2649e-01,  8.7775e-02, -9.5513e-02,  5.5818e-01,  3.4749e-01,\n",
       "          2.0180e-01, -1.1238e-01,  1.9161e-01, -3.0565e-01, -2.0964e-01,\n",
       "         -4.4672e-02,  2.3034e-01,  4.3106e-01,  1.9709e-01,  4.3687e-02,\n",
       "         -2.4458e-01, -1.5755e-01,  1.3432e-02, -3.1601e-01,  1.5329e-01,\n",
       "          4.8763e-02,  1.2542e-01,  2.0853e-02,  1.5893e-01, -1.8888e-01,\n",
       "          8.2009e-02,  2.2543e-01, -7.8105e-02,  3.1630e-01, -1.2404e-02,\n",
       "          2.0131e-03, -1.6965e-01, -1.8301e-01, -2.5454e-02,  2.1364e-01,\n",
       "         -2.8294e-01, -3.4750e-01, -1.0722e-01,  8.9463e-02, -3.4777e-01,\n",
       "         -3.9262e-01, -2.8018e-01,  2.4927e-01, -2.7758e-02, -8.2589e-02,\n",
       "          3.7620e-01,  1.2726e-02,  1.2967e-01,  1.8852e-01, -1.1819e-03,\n",
       "          2.0880e-01,  1.0359e-01, -1.3562e-01, -1.7167e-01, -2.3289e-01,\n",
       "         -3.1314e-02,  2.1511e-01, -2.5769e-01, -2.2259e-01,  1.3712e-01,\n",
       "          1.9569e-03,  3.9015e-01,  3.4855e-01, -4.2666e-01,  1.6344e-01,\n",
       "         -6.2808e-02, -4.0345e-01, -3.6256e-01,  1.1716e-01,  1.4807e-01,\n",
       "         -2.2199e-01,  2.9735e-01,  8.4796e-02,  1.5041e-01, -1.1895e-01,\n",
       "         -4.6867e-02,  1.1381e-01,  1.1149e-01, -4.2961e-01,  2.2292e-01,\n",
       "         -6.0253e-02, -2.3894e-01,  2.1047e-02, -2.6193e-01,  2.3527e-02,\n",
       "         -4.7230e-03,  2.0919e-02, -1.2008e-01, -2.6301e-01,  3.8392e-02,\n",
       "         -6.7603e-02,  1.3060e-01,  8.3526e-03,  3.3329e-01, -1.2099e-01,\n",
       "          4.8915e-03, -3.6228e-01,  2.7855e-01, -4.7591e-01,  2.4194e-01,\n",
       "          1.2440e-01, -3.7618e-02, -6.7148e-02,  6.3559e-02, -1.8841e-01,\n",
       "          4.4416e-01, -1.4926e-01, -8.1105e-02,  2.8188e-02,  8.0830e-01,\n",
       "          3.5613e-01, -1.9389e-01, -7.1083e-02,  9.8161e-02,  8.9813e-02,\n",
       "          3.9862e-01,  2.1917e-01,  1.1147e-01,  1.4718e-01,  2.1879e-01,\n",
       "          2.4170e-01,  1.0394e-01, -2.4546e-03, -4.2483e-01, -7.9739e-02,\n",
       "         -1.7576e-01,  2.4444e-01,  1.8741e-01,  3.0914e-01, -1.4741e-02,\n",
       "          1.4923e-01, -1.9934e-02, -1.9473e-01,  4.8741e-01,  3.2496e-02,\n",
       "          7.0593e-02, -2.6086e-02,  1.3068e-03,  1.0915e-01,  2.9781e-01,\n",
       "         -2.3535e-01,  1.3487e-01,  7.8618e-02,  2.0146e-01,  4.4307e-02,\n",
       "          4.7482e-03, -2.0152e-01,  4.0093e-02,  4.5272e-02, -3.9391e-02,\n",
       "         -3.3619e-01,  1.7854e-01, -3.2479e-02, -4.0290e-01,  4.2975e-01,\n",
       "          4.9981e-02, -1.2031e-01, -8.2048e-02, -2.0077e-01,  8.7201e-02,\n",
       "         -3.5493e-01, -1.5035e-01, -1.3907e-01,  2.8356e-01, -3.9206e-02,\n",
       "         -6.4041e-02,  3.2187e-01, -2.4070e-02, -1.0391e-01,  1.8451e-01,\n",
       "         -1.8121e-01, -1.6798e-01,  1.2651e-01, -2.0090e-01,  1.2892e-02,\n",
       "         -7.3372e-02, -2.0735e-01, -2.6272e-01, -1.2136e-01, -1.8146e-01,\n",
       "         -1.8582e-01, -2.2943e-01,  3.4516e-01,  1.7986e-01, -2.6982e-01,\n",
       "          2.0889e-03,  3.4393e-01,  1.9048e-01, -1.8334e-02,  1.4985e-01,\n",
       "          4.3423e-01,  2.4627e-01,  1.5448e-01,  3.1895e-02,  2.8689e-01,\n",
       "          1.0090e-01, -2.4668e-02,  7.5879e-02, -4.7115e-01, -8.5132e-02,\n",
       "          1.6577e-01, -4.6913e-01, -1.0151e-01, -1.9439e-01, -1.7505e-01,\n",
       "         -1.4454e-01, -2.0291e-01,  6.2733e-02,  5.2079e-02, -2.3870e-01,\n",
       "         -8.3910e-02, -2.8251e-01,  2.0957e-02, -2.0414e-01, -3.2586e-01,\n",
       "         -4.6830e-01,  7.0758e-02,  3.8025e-01,  8.4397e-02,  2.9290e-01,\n",
       "          2.3341e-01,  3.4036e-01,  5.5118e-02,  9.6423e-02, -1.9851e-01,\n",
       "          4.8082e-01, -1.7818e-03, -1.0237e-01,  4.0472e-02, -1.6975e-01,\n",
       "          5.0454e-02, -2.8757e-03, -1.8305e-01, -2.2970e-01, -2.8874e-01,\n",
       "          1.1409e-01, -8.3985e-02, -2.8313e-01, -4.4806e-02,  1.7149e-01,\n",
       "          5.1615e-01, -4.3205e-03,  5.2680e-01, -1.3167e-01, -2.8251e-01,\n",
       "         -1.9264e-01, -7.0072e-02,  2.8320e-01,  2.2360e-02,  2.2537e-01,\n",
       "         -2.2244e-01, -4.1397e-02,  4.4361e-01,  1.8968e-01, -2.3333e-01,\n",
       "          3.3409e-01, -3.6501e-01,  7.2367e-02,  2.1039e-01,  1.4103e-01,\n",
       "          5.3128e-02, -1.0514e-01,  7.4303e-02,  1.7183e-01,  3.4162e-01,\n",
       "         -5.2918e-01,  3.1318e-01,  2.2071e-01, -1.3845e-02, -1.6419e-01,\n",
       "          5.3146e-02, -5.1600e-02,  5.5332e-02,  8.6749e-03, -1.1735e-01,\n",
       "          1.0003e-02,  1.1031e-01, -1.1422e-02,  2.7073e-01,  1.0529e-02,\n",
       "         -3.2491e-01, -6.3502e-01, -9.4079e-02, -1.6596e-01,  1.7745e-01,\n",
       "          3.8387e-02,  3.3745e-01,  1.7702e-01, -5.7140e-02,  2.9125e-01,\n",
       "         -1.6758e-01, -2.4639e-01,  1.7356e-01, -1.4671e-01,  2.7804e-02,\n",
       "          2.4823e-01,  4.8519e-01,  7.0906e-02, -2.5029e-02,  3.2381e-02,\n",
       "          6.8481e-01,  1.9228e-01, -4.1351e-01, -5.2860e-02, -6.4072e-03,\n",
       "         -1.4395e-01,  2.9642e-01, -6.3021e-02, -3.5110e-03,  2.2752e-01,\n",
       "          2.3524e-02, -7.1021e-02, -8.6962e-02, -3.1653e-01,  3.1094e-01,\n",
       "          2.7238e-01,  1.2281e-01,  2.6214e-01, -8.0356e-02,  2.7360e-01,\n",
       "         -6.3138e-02,  1.0696e-01, -8.0561e-02,  1.7413e-01,  2.6673e-01,\n",
       "          8.6829e-02,  2.5381e-02, -1.0438e-02, -1.6491e-01,  2.5152e-02,\n",
       "         -1.2425e-01,  1.5294e-01,  6.2297e-01, -1.6544e-01,  2.5242e-02,\n",
       "          2.0059e-02,  1.6206e-01,  2.5638e-01,  4.7537e-02,  1.5371e-01,\n",
       "          1.7017e-01,  3.7500e-01, -2.4917e-01, -2.2108e-01, -5.0506e-02,\n",
       "         -1.9086e-01, -2.6073e-02, -6.0861e-02, -5.7565e-02,  8.8615e-02,\n",
       "         -2.5001e-01,  4.1993e-01,  9.0411e-02, -9.9684e-02,  8.9090e-02,\n",
       "          1.5367e-01,  4.5094e-02, -1.9735e-01,  7.9675e-02, -4.6969e-02,\n",
       "         -2.6313e-01, -7.8857e-02, -8.1283e-02, -1.1957e-01, -7.4684e-02,\n",
       "         -2.6070e-01,  2.1135e-01, -2.8239e-01,  6.4386e-02,  9.2708e-02,\n",
       "         -1.0693e-01,  1.8849e-01, -9.7032e-02,  1.4569e-01,  1.1685e-01,\n",
       "         -1.3559e-01, -2.1201e-01,  2.6891e-01,  1.0580e-01,  2.4195e-01,\n",
       "          4.1906e-01,  1.7270e-01,  2.2151e-01, -6.8326e-01,  3.3506e-01,\n",
       "         -3.5631e-03,  1.3562e-01,  3.7168e-02, -3.5251e-02,  1.1998e-01,\n",
       "          7.9539e-02,  5.9414e-01, -6.6101e-02, -2.6764e-01,  8.6729e-02,\n",
       "          8.4871e-02,  3.2380e-01,  5.3051e-02, -2.2970e-01,  2.4535e-01,\n",
       "          6.0136e-02, -6.6690e-02, -1.7108e-01,  1.2503e-01,  1.1589e-01,\n",
       "         -1.2742e-01, -2.7280e-01, -2.7641e-01, -4.1014e-01,  1.8488e-01,\n",
       "          2.6689e-01, -8.7033e-02, -1.1259e-01,  2.6173e-01,  1.9678e-01,\n",
       "          1.9438e-01, -1.6329e-01, -2.6362e-01,  6.0227e-02,  1.3523e-01,\n",
       "          5.8761e-01,  2.6933e-01, -1.4830e-01,  1.1074e-01, -3.9199e-03,\n",
       "         -1.1484e-01,  2.6356e-01, -2.0317e-01, -4.0415e-02,  4.0170e-02,\n",
       "         -3.2816e-01,  2.5316e-01, -2.2422e-01, -1.9102e-01,  1.3313e-01,\n",
       "         -4.1663e-02, -2.5001e-01,  3.5616e-01,  1.2755e-01,  8.2899e-02,\n",
       "         -2.7446e-01,  1.0383e-01, -4.6638e-02,  9.4892e-02,  2.9654e-02,\n",
       "         -1.5793e-01, -1.1209e-01, -3.1278e-01, -1.0576e-01, -7.0907e-02,\n",
       "         -2.3963e-01, -3.8093e-01,  1.0195e-01,  1.2793e-01,  8.5967e-02,\n",
       "          2.4253e-01,  1.8455e-01, -3.4341e-02, -2.4710e-02,  3.2862e-03,\n",
       "          6.6704e-02,  1.7725e-01, -1.2476e-01, -2.5221e-01,  3.6072e-01,\n",
       "          7.7159e-02, -4.1562e-01, -9.8198e-02, -2.1317e-01, -1.9340e-01,\n",
       "         -5.3311e-02, -1.7596e-01, -2.1561e-01,  1.1695e-01, -3.0787e-01,\n",
       "         -1.0040e-01, -5.4012e-01, -1.2116e-01, -3.8114e-01, -3.1814e-01,\n",
       "         -1.3414e-01, -2.4283e-01, -9.5880e-02, -1.1843e-01, -2.3932e-01,\n",
       "          1.9238e-01,  9.6888e-02, -1.4315e-02,  8.8045e-02, -2.6841e-01,\n",
       "         -2.8972e-01, -8.3002e-03, -7.0300e-04,  1.3081e-01,  2.9951e-01,\n",
       "         -4.4077e-01,  3.3922e-01, -3.6745e-02,  2.0281e-01,  2.9925e-02,\n",
       "          2.7569e-01,  1.2781e-01, -4.0442e-01,  3.9249e-01, -8.0817e-02,\n",
       "          4.8675e-02,  2.9842e-01,  1.7932e-01,  3.1794e-01, -3.6154e-02,\n",
       "          4.8831e-02, -1.4973e-01,  2.1152e-01, -4.4790e-02, -1.7745e-01]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import quant_vision_transformer\n",
    "import quant_swin_transformer\n",
    "import swin_transformer\n",
    "import torch\n",
    "import timm\n",
    "# model = quant_vision_transformer.lowbit_VisionTransformer(4)\n",
    "model = quant_swin_transformer.SwinTransformer(4)\n",
    "# print(model2)\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "# print(model(x))\n",
    "\n",
    "# \n",
    "def print_output_size(module, input, output):\n",
    "    if isinstance(output, tuple):\n",
    "        print(f\"{module.__class__.__name__}: {output[0].size()}\")\n",
    "        print(f\"{module.__class__.__name__}: {output[1].size()}\")\n",
    "    else:\n",
    "        print(f\"{module.__class__.__name__}: {output.size()}\")\n",
    "\n",
    "# \n",
    "hooks = []\n",
    "for module in model.modules():\n",
    "    hook = module.register_forward_hook(print_output_size)\n",
    "    hooks.append(hook)\n",
    "\n",
    "# \n",
    "input_data = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# \n",
    "model(input_data)\n",
    "print(model)\n",
    "# print(model)\n",
    "\n",
    "# \n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "model(x)\n",
    "# from quant_vision_transformer import *\n",
    "# model2= swin_transformer.lowbit_VisionTransformer(nbits=4, patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True)\n",
    "# model2_params = sum(p.numel() for p in model2.parameters())\n",
    "# print(f\"Total number of parameters in LeNet-5: {model2_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformer(\n",
      "  (patch_embed): Q_PatchEmbed(\n",
      "    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (layers): Sequential(\n",
      "    (0): Q_SwinTransformerStage(\n",
      "      (downsample): Identity()\n",
      "      (blocks): Sequential(\n",
      "        (0): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=96, out_features=288, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=96, out_features=96, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=96, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=384, out_features=96, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (1): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=96, out_features=288, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=96, out_features=96, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.009)\n",
      "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=96, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=384, out_features=96, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.009)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Q_SwinTransformerStage(\n",
      "      (downsample): Q_PatchMerging(\n",
      "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (reduction): LinearQ(\n",
      "          in_features=384, out_features=96, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "          (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "        )\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=192, out_features=576, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=192, out_features=192, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.018)\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=192, out_features=768, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=768, out_features=192, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.018)\n",
      "        )\n",
      "        (1): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=192, out_features=576, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=192, out_features=192, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.027)\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=192, out_features=768, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=768, out_features=192, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.027)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Q_SwinTransformerStage(\n",
      "      (downsample): Q_PatchMerging(\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (reduction): LinearQ(\n",
      "          in_features=768, out_features=192, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "          (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "        )\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=384, out_features=1152, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=384, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.036)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=384, out_features=1536, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=1536, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.036)\n",
      "        )\n",
      "        (1): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=384, out_features=1152, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=384, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.045)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=384, out_features=1536, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=1536, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.045)\n",
      "        )\n",
      "        (2): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=384, out_features=1152, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=384, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.055)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=384, out_features=1536, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=1536, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.055)\n",
      "        )\n",
      "        (3): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=384, out_features=1152, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=384, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.064)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=384, out_features=1536, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=1536, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.064)\n",
      "        )\n",
      "        (4): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=384, out_features=1152, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=384, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.073)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=384, out_features=1536, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=1536, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.073)\n",
      "        )\n",
      "        (5): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=384, out_features=1152, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=384, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.082)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=384, out_features=1536, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=1536, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.082)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Q_SwinTransformerStage(\n",
      "      (downsample): Q_PatchMerging(\n",
      "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        (reduction): LinearQ(\n",
      "          in_features=1536, out_features=384, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "          (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "        )\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=768, out_features=2304, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=768, out_features=768, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.091)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=768, out_features=3072, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=3072, out_features=768, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.091)\n",
      "        )\n",
      "        (1): Q_SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Q_WindowAttention(\n",
      "            (norm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (qkv): LinearQ(\n",
      "              in_features=768, out_features=2304, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): LinearQ(\n",
      "              in_features=768, out_features=768, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (q_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (k_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (v_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (attn_act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.100)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Q_Mlp(\n",
      "            (fc1): LinearQ(\n",
      "              in_features=768, out_features=3072, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): LinearQ(\n",
      "              in_features=3072, out_features=768, bias=True, {'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>}\n",
      "              (act): ActQ({'nbits': 4, 'mode': <Qmodes.kernel_wise: 2>})\n",
      "            )\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): ClassifierHead(\n",
      "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (fc): LinearQ(\n",
      "      in_features=768, out_features=1000, bias=True, fake\n",
      "      (act): ActQ(fake)\n",
      "    )\n",
      "    (flatten): Identity()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d: torch.Size([1, 96, 56, 56])\n",
      "LayerNorm: torch.Size([1, 56, 56, 96])\n",
      "PatchEmbed: torch.Size([1, 56, 56, 96])\n",
      "Identity: torch.Size([1, 56, 56, 96])\n",
      "LayerNorm: torch.Size([1, 56, 56, 96])\n",
      "Linear: torch.Size([64, 49, 288])\n",
      "Softmax: torch.Size([64, 3, 49, 49])\n",
      "Dropout: torch.Size([64, 3, 49, 49])\n",
      "Linear: torch.Size([64, 49, 96])\n",
      "Dropout: torch.Size([64, 49, 96])\n",
      "WindowAttention: torch.Size([64, 49, 96])\n",
      "Identity: torch.Size([1, 56, 56, 96])\n",
      "LayerNorm: torch.Size([1, 3136, 96])\n",
      "Linear: torch.Size([1, 3136, 384])\n",
      "GELU: torch.Size([1, 3136, 384])\n",
      "Dropout: torch.Size([1, 3136, 384])\n",
      "Identity: torch.Size([1, 3136, 384])\n",
      "Linear: torch.Size([1, 3136, 96])\n",
      "Dropout: torch.Size([1, 3136, 96])\n",
      "Mlp: torch.Size([1, 3136, 96])\n",
      "Identity: torch.Size([1, 3136, 96])\n",
      "SwinTransformerBlock: torch.Size([1, 56, 56, 96])\n",
      "LayerNorm: torch.Size([1, 56, 56, 96])\n",
      "Linear: torch.Size([64, 49, 288])\n",
      "Softmax: torch.Size([64, 3, 49, 49])\n",
      "Dropout: torch.Size([64, 3, 49, 49])\n",
      "Linear: torch.Size([64, 49, 96])\n",
      "Dropout: torch.Size([64, 49, 96])\n",
      "WindowAttention: torch.Size([64, 49, 96])\n",
      "DropPath: torch.Size([1, 56, 56, 96])\n",
      "LayerNorm: torch.Size([1, 3136, 96])\n",
      "Linear: torch.Size([1, 3136, 384])\n",
      "GELU: torch.Size([1, 3136, 384])\n",
      "Dropout: torch.Size([1, 3136, 384])\n",
      "Identity: torch.Size([1, 3136, 384])\n",
      "Linear: torch.Size([1, 3136, 96])\n",
      "Dropout: torch.Size([1, 3136, 96])\n",
      "Mlp: torch.Size([1, 3136, 96])\n",
      "DropPath: torch.Size([1, 3136, 96])\n",
      "SwinTransformerBlock: torch.Size([1, 56, 56, 96])\n",
      "Sequential: torch.Size([1, 56, 56, 96])\n",
      "SwinTransformerStage: torch.Size([1, 56, 56, 96])\n",
      "LayerNorm: torch.Size([1, 28, 28, 384])\n",
      "Linear: torch.Size([1, 28, 28, 192])\n",
      "PatchMerging: torch.Size([1, 28, 28, 192])\n",
      "LayerNorm: torch.Size([1, 28, 28, 192])\n",
      "Linear: torch.Size([16, 49, 576])\n",
      "Softmax: torch.Size([16, 6, 49, 49])\n",
      "Dropout: torch.Size([16, 6, 49, 49])\n",
      "Linear: torch.Size([16, 49, 192])\n",
      "Dropout: torch.Size([16, 49, 192])\n",
      "WindowAttention: torch.Size([16, 49, 192])\n",
      "DropPath: torch.Size([1, 28, 28, 192])\n",
      "LayerNorm: torch.Size([1, 784, 192])\n",
      "Linear: torch.Size([1, 784, 768])\n",
      "GELU: torch.Size([1, 784, 768])\n",
      "Dropout: torch.Size([1, 784, 768])\n",
      "Identity: torch.Size([1, 784, 768])\n",
      "Linear: torch.Size([1, 784, 192])\n",
      "Dropout: torch.Size([1, 784, 192])\n",
      "Mlp: torch.Size([1, 784, 192])\n",
      "DropPath: torch.Size([1, 784, 192])\n",
      "SwinTransformerBlock: torch.Size([1, 28, 28, 192])\n",
      "LayerNorm: torch.Size([1, 28, 28, 192])\n",
      "Linear: torch.Size([16, 49, 576])\n",
      "Softmax: torch.Size([16, 6, 49, 49])\n",
      "Dropout: torch.Size([16, 6, 49, 49])\n",
      "Linear: torch.Size([16, 49, 192])\n",
      "Dropout: torch.Size([16, 49, 192])\n",
      "WindowAttention: torch.Size([16, 49, 192])\n",
      "DropPath: torch.Size([1, 28, 28, 192])\n",
      "LayerNorm: torch.Size([1, 784, 192])\n",
      "Linear: torch.Size([1, 784, 768])\n",
      "GELU: torch.Size([1, 784, 768])\n",
      "Dropout: torch.Size([1, 784, 768])\n",
      "Identity: torch.Size([1, 784, 768])\n",
      "Linear: torch.Size([1, 784, 192])\n",
      "Dropout: torch.Size([1, 784, 192])\n",
      "Mlp: torch.Size([1, 784, 192])\n",
      "DropPath: torch.Size([1, 784, 192])\n",
      "SwinTransformerBlock: torch.Size([1, 28, 28, 192])\n",
      "Sequential: torch.Size([1, 28, 28, 192])\n",
      "SwinTransformerStage: torch.Size([1, 28, 28, 192])\n",
      "LayerNorm: torch.Size([1, 14, 14, 768])\n",
      "Linear: torch.Size([1, 14, 14, 384])\n",
      "PatchMerging: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 14, 14, 384])\n",
      "Linear: torch.Size([4, 49, 1152])\n",
      "Softmax: torch.Size([4, 12, 49, 49])\n",
      "Dropout: torch.Size([4, 12, 49, 49])\n",
      "Linear: torch.Size([4, 49, 384])\n",
      "Dropout: torch.Size([4, 49, 384])\n",
      "WindowAttention: torch.Size([4, 49, 384])\n",
      "DropPath: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 196, 384])\n",
      "Linear: torch.Size([1, 196, 1536])\n",
      "GELU: torch.Size([1, 196, 1536])\n",
      "Dropout: torch.Size([1, 196, 1536])\n",
      "Identity: torch.Size([1, 196, 1536])\n",
      "Linear: torch.Size([1, 196, 384])\n",
      "Dropout: torch.Size([1, 196, 384])\n",
      "Mlp: torch.Size([1, 196, 384])\n",
      "DropPath: torch.Size([1, 196, 384])\n",
      "SwinTransformerBlock: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 14, 14, 384])\n",
      "Linear: torch.Size([4, 49, 1152])\n",
      "Softmax: torch.Size([4, 12, 49, 49])\n",
      "Dropout: torch.Size([4, 12, 49, 49])\n",
      "Linear: torch.Size([4, 49, 384])\n",
      "Dropout: torch.Size([4, 49, 384])\n",
      "WindowAttention: torch.Size([4, 49, 384])\n",
      "DropPath: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 196, 384])\n",
      "Linear: torch.Size([1, 196, 1536])\n",
      "GELU: torch.Size([1, 196, 1536])\n",
      "Dropout: torch.Size([1, 196, 1536])\n",
      "Identity: torch.Size([1, 196, 1536])\n",
      "Linear: torch.Size([1, 196, 384])\n",
      "Dropout: torch.Size([1, 196, 384])\n",
      "Mlp: torch.Size([1, 196, 384])\n",
      "DropPath: torch.Size([1, 196, 384])\n",
      "SwinTransformerBlock: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 14, 14, 384])\n",
      "Linear: torch.Size([4, 49, 1152])\n",
      "Softmax: torch.Size([4, 12, 49, 49])\n",
      "Dropout: torch.Size([4, 12, 49, 49])\n",
      "Linear: torch.Size([4, 49, 384])\n",
      "Dropout: torch.Size([4, 49, 384])\n",
      "WindowAttention: torch.Size([4, 49, 384])\n",
      "DropPath: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 196, 384])\n",
      "Linear: torch.Size([1, 196, 1536])\n",
      "GELU: torch.Size([1, 196, 1536])\n",
      "Dropout: torch.Size([1, 196, 1536])\n",
      "Identity: torch.Size([1, 196, 1536])\n",
      "Linear: torch.Size([1, 196, 384])\n",
      "Dropout: torch.Size([1, 196, 384])\n",
      "Mlp: torch.Size([1, 196, 384])\n",
      "DropPath: torch.Size([1, 196, 384])\n",
      "SwinTransformerBlock: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 14, 14, 384])\n",
      "Linear: torch.Size([4, 49, 1152])\n",
      "Softmax: torch.Size([4, 12, 49, 49])\n",
      "Dropout: torch.Size([4, 12, 49, 49])\n",
      "Linear: torch.Size([4, 49, 384])\n",
      "Dropout: torch.Size([4, 49, 384])\n",
      "WindowAttention: torch.Size([4, 49, 384])\n",
      "DropPath: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 196, 384])\n",
      "Linear: torch.Size([1, 196, 1536])\n",
      "GELU: torch.Size([1, 196, 1536])\n",
      "Dropout: torch.Size([1, 196, 1536])\n",
      "Identity: torch.Size([1, 196, 1536])\n",
      "Linear: torch.Size([1, 196, 384])\n",
      "Dropout: torch.Size([1, 196, 384])\n",
      "Mlp: torch.Size([1, 196, 384])\n",
      "DropPath: torch.Size([1, 196, 384])\n",
      "SwinTransformerBlock: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 14, 14, 384])\n",
      "Linear: torch.Size([4, 49, 1152])\n",
      "Softmax: torch.Size([4, 12, 49, 49])\n",
      "Dropout: torch.Size([4, 12, 49, 49])\n",
      "Linear: torch.Size([4, 49, 384])\n",
      "Dropout: torch.Size([4, 49, 384])\n",
      "WindowAttention: torch.Size([4, 49, 384])\n",
      "DropPath: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 196, 384])\n",
      "Linear: torch.Size([1, 196, 1536])\n",
      "GELU: torch.Size([1, 196, 1536])\n",
      "Dropout: torch.Size([1, 196, 1536])\n",
      "Identity: torch.Size([1, 196, 1536])\n",
      "Linear: torch.Size([1, 196, 384])\n",
      "Dropout: torch.Size([1, 196, 384])\n",
      "Mlp: torch.Size([1, 196, 384])\n",
      "DropPath: torch.Size([1, 196, 384])\n",
      "SwinTransformerBlock: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 14, 14, 384])\n",
      "Linear: torch.Size([4, 49, 1152])\n",
      "Softmax: torch.Size([4, 12, 49, 49])\n",
      "Dropout: torch.Size([4, 12, 49, 49])\n",
      "Linear: torch.Size([4, 49, 384])\n",
      "Dropout: torch.Size([4, 49, 384])\n",
      "WindowAttention: torch.Size([4, 49, 384])\n",
      "DropPath: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 196, 384])\n",
      "Linear: torch.Size([1, 196, 1536])\n",
      "GELU: torch.Size([1, 196, 1536])\n",
      "Dropout: torch.Size([1, 196, 1536])\n",
      "Identity: torch.Size([1, 196, 1536])\n",
      "Linear: torch.Size([1, 196, 384])\n",
      "Dropout: torch.Size([1, 196, 384])\n",
      "Mlp: torch.Size([1, 196, 384])\n",
      "DropPath: torch.Size([1, 196, 384])\n",
      "SwinTransformerBlock: torch.Size([1, 14, 14, 384])\n",
      "Sequential: torch.Size([1, 14, 14, 384])\n",
      "SwinTransformerStage: torch.Size([1, 14, 14, 384])\n",
      "LayerNorm: torch.Size([1, 7, 7, 1536])\n",
      "Linear: torch.Size([1, 7, 7, 768])\n",
      "PatchMerging: torch.Size([1, 7, 7, 768])\n",
      "LayerNorm: torch.Size([1, 7, 7, 768])\n",
      "Linear: torch.Size([1, 49, 2304])\n",
      "Softmax: torch.Size([1, 24, 49, 49])\n",
      "Dropout: torch.Size([1, 24, 49, 49])\n",
      "Linear: torch.Size([1, 49, 768])\n",
      "Dropout: torch.Size([1, 49, 768])\n",
      "WindowAttention: torch.Size([1, 49, 768])\n",
      "DropPath: torch.Size([1, 7, 7, 768])\n",
      "LayerNorm: torch.Size([1, 49, 768])\n",
      "Linear: torch.Size([1, 49, 3072])\n",
      "GELU: torch.Size([1, 49, 3072])\n",
      "Dropout: torch.Size([1, 49, 3072])\n",
      "Identity: torch.Size([1, 49, 3072])\n",
      "Linear: torch.Size([1, 49, 768])\n",
      "Dropout: torch.Size([1, 49, 768])\n",
      "Mlp: torch.Size([1, 49, 768])\n",
      "DropPath: torch.Size([1, 49, 768])\n",
      "SwinTransformerBlock: torch.Size([1, 7, 7, 768])\n",
      "LayerNorm: torch.Size([1, 7, 7, 768])\n",
      "Linear: torch.Size([1, 49, 2304])\n",
      "Softmax: torch.Size([1, 24, 49, 49])\n",
      "Dropout: torch.Size([1, 24, 49, 49])\n",
      "Linear: torch.Size([1, 49, 768])\n",
      "Dropout: torch.Size([1, 49, 768])\n",
      "WindowAttention: torch.Size([1, 49, 768])\n",
      "DropPath: torch.Size([1, 7, 7, 768])\n",
      "LayerNorm: torch.Size([1, 49, 768])\n",
      "Linear: torch.Size([1, 49, 3072])\n",
      "GELU: torch.Size([1, 49, 3072])\n",
      "Dropout: torch.Size([1, 49, 3072])\n",
      "Identity: torch.Size([1, 49, 3072])\n",
      "Linear: torch.Size([1, 49, 768])\n",
      "Dropout: torch.Size([1, 49, 768])\n",
      "Mlp: torch.Size([1, 49, 768])\n",
      "DropPath: torch.Size([1, 49, 768])\n",
      "SwinTransformerBlock: torch.Size([1, 7, 7, 768])\n",
      "Sequential: torch.Size([1, 7, 7, 768])\n",
      "SwinTransformerStage: torch.Size([1, 7, 7, 768])\n",
      "Sequential: torch.Size([1, 7, 7, 768])\n",
      "LayerNorm: torch.Size([1, 7, 7, 768])\n",
      "FastAdaptiveAvgPool: torch.Size([1, 768])\n",
      "Identity: torch.Size([1, 768])\n",
      "SelectAdaptivePool2d: torch.Size([1, 768])\n",
      "Dropout: torch.Size([1, 768])\n",
      "Linear: torch.Size([1, 1000])\n",
      "Identity: torch.Size([1, 1000])\n",
      "ClassifierHead: torch.Size([1, 1000])\n",
      "SwinTransformer: torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "import quant_vision_transformer\n",
    "import quant_swin_transformer\n",
    "import swin_transformer\n",
    "import torch\n",
    "# model = quant_vision_transformer.lowbit_VisionTransformer(4)\n",
    "model = swin_transformer.SwinTransformer()\n",
    "# print(model2)\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "# print(model(x))\n",
    "\n",
    "# \n",
    "def print_output_size(module, input, output):\n",
    "    print(f\"{module.__class__.__name__}: {output.size()}\")\n",
    "\n",
    "# \n",
    "hooks = []\n",
    "for module in model.modules():\n",
    "    hook = module.register_forward_hook(print_output_size)\n",
    "    hooks.append(hook)\n",
    "\n",
    "# \n",
    "input_data = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# \n",
    "model(input_data)\n",
    "\n",
    "# \n",
    "for hook in hooks:\n",
    "    hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\py38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_size: torch.Size([1, 3, 224, 224])\n",
      "zero_point.size() torch.Size([1, 3, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 3, 224, 224])\n",
      "Conv2dQ: torch.Size([1, 768, 14, 14])\n",
      "Q_PatchEmbed: torch.Size([1, 196, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 2304])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "Dropout: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 12, 198, 198])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Attention: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 3072])\n",
      "GELU: torch.Size([1, 198, 3072])\n",
      "Dropout: torch.Size([1, 198, 3072])\n",
      "x_size: torch.Size([1, 198, 3072])\n",
      "zero_point.size() torch.Size([3072])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 3072])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Mlp: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "Q_Block: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 2304])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "Dropout: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 12, 198, 198])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Attention: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 3072])\n",
      "GELU: torch.Size([1, 198, 3072])\n",
      "Dropout: torch.Size([1, 198, 3072])\n",
      "x_size: torch.Size([1, 198, 3072])\n",
      "zero_point.size() torch.Size([3072])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 3072])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Mlp: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "Q_Block: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 2304])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "Dropout: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 12, 198, 198])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Attention: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 3072])\n",
      "GELU: torch.Size([1, 198, 3072])\n",
      "Dropout: torch.Size([1, 198, 3072])\n",
      "x_size: torch.Size([1, 198, 3072])\n",
      "zero_point.size() torch.Size([3072])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 3072])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Mlp: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "Q_Block: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 2304])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "Dropout: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 12, 198, 198])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Attention: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 3072])\n",
      "GELU: torch.Size([1, 198, 3072])\n",
      "Dropout: torch.Size([1, 198, 3072])\n",
      "x_size: torch.Size([1, 198, 3072])\n",
      "zero_point.size() torch.Size([3072])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 3072])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Mlp: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "Q_Block: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 2304])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "Dropout: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 12, 198, 198])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Attention: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 3072])\n",
      "GELU: torch.Size([1, 198, 3072])\n",
      "Dropout: torch.Size([1, 198, 3072])\n",
      "x_size: torch.Size([1, 198, 3072])\n",
      "zero_point.size() torch.Size([3072])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 3072])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Mlp: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "Q_Block: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 2304])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "Dropout: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 12, 198, 198])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Attention: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 3072])\n",
      "GELU: torch.Size([1, 198, 3072])\n",
      "Dropout: torch.Size([1, 198, 3072])\n",
      "x_size: torch.Size([1, 198, 3072])\n",
      "zero_point.size() torch.Size([3072])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 3072])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Mlp: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "Q_Block: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 2304])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "Dropout: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 12, 198, 198])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Attention: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 3072])\n",
      "GELU: torch.Size([1, 198, 3072])\n",
      "Dropout: torch.Size([1, 198, 3072])\n",
      "x_size: torch.Size([1, 198, 3072])\n",
      "zero_point.size() torch.Size([3072])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 3072])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Mlp: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "Q_Block: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 2304])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "Dropout: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 12, 198, 198])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Attention: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 3072])\n",
      "GELU: torch.Size([1, 198, 3072])\n",
      "Dropout: torch.Size([1, 198, 3072])\n",
      "x_size: torch.Size([1, 198, 3072])\n",
      "zero_point.size() torch.Size([3072])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 3072])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Mlp: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "Q_Block: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 2304])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "Dropout: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 12, 198, 198])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Attention: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 3072])\n",
      "GELU: torch.Size([1, 198, 3072])\n",
      "Dropout: torch.Size([1, 198, 3072])\n",
      "x_size: torch.Size([1, 198, 3072])\n",
      "zero_point.size() torch.Size([3072])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 3072])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Mlp: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "Q_Block: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 2304])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "Dropout: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 12, 198, 198])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Attention: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 3072])\n",
      "GELU: torch.Size([1, 198, 3072])\n",
      "Dropout: torch.Size([1, 198, 3072])\n",
      "x_size: torch.Size([1, 198, 3072])\n",
      "zero_point.size() torch.Size([3072])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 3072])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Mlp: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "Q_Block: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 2304])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "Dropout: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 12, 198, 198])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Attention: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 3072])\n",
      "GELU: torch.Size([1, 198, 3072])\n",
      "Dropout: torch.Size([1, 198, 3072])\n",
      "x_size: torch.Size([1, 198, 3072])\n",
      "zero_point.size() torch.Size([3072])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 3072])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Mlp: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "Q_Block: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 2304])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "LayerNorm: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "x_size: torch.Size([1, 12, 198, 64])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 64])\n",
      "Dropout: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 12, 198, 198])\n",
      "zero_point.size() torch.Size([1, 12, 1, 1])\n",
      "\n",
      "ActQ: torch.Size([1, 12, 198, 198])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Attention: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 198, 768])\n",
      "zero_point.size() torch.Size([768])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 768])\n",
      "LinearQ: torch.Size([1, 198, 3072])\n",
      "GELU: torch.Size([1, 198, 3072])\n",
      "Dropout: torch.Size([1, 198, 3072])\n",
      "x_size: torch.Size([1, 198, 3072])\n",
      "zero_point.size() torch.Size([3072])\n",
      "\n",
      "ActQ: torch.Size([1, 198, 3072])\n",
      "LinearQ: torch.Size([1, 198, 768])\n",
      "Dropout: torch.Size([1, 198, 768])\n",
      "Q_Mlp: torch.Size([1, 198, 768])\n",
      "Identity: torch.Size([1, 198, 768])\n",
      "Q_Block: torch.Size([1, 198, 768])\n",
      "Sequential: torch.Size([1, 198, 768])\n",
      "LayerNorm: torch.Size([1, 198, 768])\n",
      "x_size: torch.Size([1, 768])\n",
      "zero_point.size() torch.Size([1, 768])\n",
      "\n",
      "ActQ: torch.Size([1, 768])\n",
      "LinearQ: torch.Size([1, 1000])\n",
      "x_size: torch.Size([1, 768])\n",
      "zero_point.size() torch.Size([1, 768])\n",
      "\n",
      "ActQ: torch.Size([1, 768])\n",
      "LinearQ: torch.Size([1, 1000])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Lucy\\Desktop\\MyQSwin\\test.ipynb  4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lucy/Desktop/MyQSwin/test.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m input_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lucy/Desktop/MyQSwin/test.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Lucy/Desktop/MyQSwin/test.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m model(input_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lucy/Desktop/MyQSwin/test.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lucy/Desktop/MyQSwin/test.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1581\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1579\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args, kwargs, result)\n\u001b[0;32m   1580\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1581\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, args, result)\n\u001b[0;32m   1583\u001b[0m \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1584\u001b[0m     result \u001b[39m=\u001b[39m hook_result\n",
      "\u001b[1;32mc:\\Users\\Lucy\\Desktop\\MyQSwin\\test.ipynb  4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lucy/Desktop/MyQSwin/test.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_output_size\u001b[39m(module, \u001b[39minput\u001b[39m, output):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Lucy/Desktop/MyQSwin/test.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00moutput\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "import quant_vision_transformer\n",
    "import quant_swin_transformer\n",
    "import swin_transformer\n",
    "import torch\n",
    "# model = quant_vision_transformer.lowbit_VisionTransformer(4)\n",
    "model = quant_vision_transformer.lowbit_VisionTransformer(4)\n",
    "# print(model2)\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "# print(model(x))\n",
    "\n",
    "# \n",
    "def print_output_size(module, input, output):\n",
    "    print(f\"{module.__class__.__name__}: {output.size()}\")\n",
    "\n",
    "# \n",
    "hooks = []\n",
    "for module in model.modules():\n",
    "    hook = module.register_forward_hook(print_output_size)\n",
    "    hooks.append(hook)\n",
    "\n",
    "# \n",
    "input_data = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# \n",
    "model(input_data)\n",
    "\n",
    "# \n",
    "for hook in hooks:\n",
    "    hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\py38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from main import * \n",
    "import quant_swin_transformer\n",
    "import swin_transformer\n",
    "from typing import Iterable, Optional\n",
    "import torch\n",
    "import math\n",
    "import sys\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model: torch.nn.Module, teacher_model: torch.nn.Module, criterion: DistillationLoss,\n",
    "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
    "                    device: torch.device, epoch: int, loss_scaler, max_norm: float = 0,\n",
    "                    model_ema: Optional[ModelEma] = None, mixup_fn: Optional[Mixup] = None,\n",
    "                    set_training_mode=True):\n",
    "    model.train(set_training_mode)\n",
    "    student_feature = None\n",
    "\n",
    "    def hook_fn_1(module, input, output):\n",
    "        #  nonlocal \n",
    "        global student_feature\n",
    "        # \n",
    "        student_feature = output\n",
    "    \n",
    "\n",
    "\n",
    "    teacher_model.eval()\n",
    "    # hook_handle = model.layers[-1].blocks[1].attn.register_forward_hook(hook_fn)\n",
    "    model.layers[-1].blocks[1].attn.register_forward_hook(hook_fn_1)\n",
    "\n",
    "    \n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 10\n",
    "\n",
    "    for samples, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        samples = samples.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        # if mixup_fn is not None:\n",
    "        #     samples, targets = mixup_fn(samples, targets)\n",
    "\n",
    "        outputs = model(samples)\n",
    "        # hook_handle.remove()\n",
    "        print(student_feature[0,0,0])\n",
    "        loss = criterion(samples, outputs, targets,student_feature)\n",
    "\n",
    "\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # this attribute is added by timm on one optimizer (adahessian)\n",
    "        # is_second_order = hasattr(optimizer, 'is_second_order') and optimizer.is_second_order\n",
    "        # loss_scaler(loss, optimizer, clip_grad=max_norm,\n",
    "        #             parameters=model.parameters(), create_graph=is_second_order)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        # if model_ema is not None:\n",
    "        #     model_ema.update(model)\n",
    "\n",
    "        metric_logger.update(loss=loss_value)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class DistillationLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This module wraps a standard criterion and adds an extra knowledge distillation loss by\n",
    "    taking a teacher model prediction and using it as additional supervision.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_criterion: torch.nn.Module, s_t_criterion: torch.nn.Module,teacher_model: torch.nn.Module,\n",
    "                 distillation_type: str, alpha: float, tau: float):\n",
    "        super().__init__()\n",
    "        self.base_criterion = base_criterion\n",
    "        self.s_t_criterion = s_t_criterion\n",
    "        self.teacher_model = teacher_model\n",
    "        assert distillation_type in ['none', 'soft', 'hard']\n",
    "        self.distillation_type = distillation_type\n",
    "        self.alpha = alpha\n",
    "        self.tau = tau\n",
    "\n",
    "    def forward(self, inputs, outputs, labels, model_feature):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: The original inputs that are feed to the teacher model\n",
    "            outputs: the outputs of the model to be trained. It is expected to be\n",
    "                either a Tensor, or a Tuple[Tensor, Tensor], with the original output\n",
    "                in the first position and the distillation predictions as the second output\n",
    "            labels: the labels for the base criterion\n",
    "        \"\"\"\n",
    "        teacher_feature = None\n",
    "        def hook_fn_2(module, input, output):\n",
    "            global teacher_feature\n",
    "            teacher_feature = output\n",
    "        self.teacher_model.layers[-1].blocks[1].attn.register_forward_hook(hook_fn_2)\n",
    "\n",
    "        \n",
    "        # outputs_kd = None\n",
    "        # if not isinstance(outputs, torch.Tensor):\n",
    "        #     # assume that the model outputs a tuple of [outputs, outputs_kd]\n",
    "        #     outputs, outputs_kd = outputs\n",
    "        base_loss = self.base_criterion(outputs, labels)\n",
    "        if self.distillation_type == 'none':\n",
    "            return base_loss\n",
    "\n",
    "        # if outputs_kd is None:\n",
    "        #     raise ValueError(\"When knowledge distillation is enabled, the model is \"\n",
    "        #                      \"expected to return a Tuple[Tensor, Tensor] with the output of the \"\n",
    "        #                      \"class_token and the dist_token\")\n",
    "        # don't backprop throught the teacher\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher_model(inputs)\n",
    "\n",
    "        s_t_loss = self.s_t_criterion(model_feature,teacher_feature)\n",
    "        \n",
    "\n",
    "\n",
    "        # if self.distillation_type == 'soft':\n",
    "        #     T = self.tau\n",
    "        #     # taken from https://github.com/peterliht/knowledge-distillation-pytorch/blob/master/model/net.py#L100\n",
    "        #     # with slight modifications\n",
    "        #     distillation_loss = F.kl_div(\n",
    "        #         F.log_softmax(outputs_kd / T, dim=1),\n",
    "        #         #We provide the teacher's targets in log probability because we use log_target=True \n",
    "        #         #(as recommended in pytorch https://github.com/pytorch/pytorch/blob/9324181d0ac7b4f7949a574dbc3e8be30abe7041/torch/nn/functional.py#L2719)\n",
    "        #         #but it is possible to give just the probabilities and set log_target=False. In our experiments we tried both.\n",
    "        #         F.log_softmax(teacher_outputs / T, dim=1),\n",
    "        #         reduction='sum',\n",
    "        #         log_target=True\n",
    "        #     ) * (T * T) / outputs_kd.numel()\n",
    "        #     #We divide by outputs_kd.numel() to have the legacy PyTorch behavior. \n",
    "        #     #But we also experiments output_kd.size(0) \n",
    "        #     #see issue 61(https://github.com/facebookresearch/deit/issues/61) for more details\n",
    "        # elif self.distillation_type == 'hard':\n",
    "        #     distillation_loss = F.cross_entropy(outputs_kd, teacher_outputs.argmax(dim=1))\n",
    "\n",
    "        loss = base_loss * (1 - self.alpha) + s_t_loss * self.alpha\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = swin_transformer.SwinTransformer()\n",
    "# optimizer = create_optimizer(args, model=model)\n",
    "# print(model)\n",
    "# model1 = timm.create_model('swin_tiny_patch4_window7_224', pretrained=False)\n",
    "# print(model2)\n",
    "last_layer_features = None\n",
    "def hook_fn(module, input, output):\n",
    "    # \n",
    "    global  last_layer_features\n",
    "    last_layer_features = output[1]\n",
    "    # print(output.shape)\n",
    "# model2.layers[-1].blocks[1].attn.register_forward_hook(hook_fn)\n",
    "model1.layers[-1].blocks[1].attn.register_forward_hook(hook_fn)\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "print(last_layer_features)\n",
    "model1(x)\n",
    "# model2(x)\n",
    "print(last_layer_features.shape)\n",
    "# print(model1)\n",
    "model1_params = sum(p.numel() for p in model1.parameters())\n",
    "print(f\"Total number of parameters in model1: {model1_params}\")\n",
    "\n",
    "\n",
    "# model2_params = sum(p.numel() for p in model2.parameters())\n",
    "# print(f\"Total number of parameters in model2: {model2_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import sys\n",
    "import quant_vision_transformer\n",
    "model1 = timm.create_model(\"vit_small_patch16_224\")\n",
    "# print(model1)\n",
    "model1_params = sum(p.numel() for p in model1.parameters())\n",
    "print(f\"Total number of parameters in model1: {model1_params}\")\n",
    "\n",
    "\n",
    "model2= quant_vision_transformer.lowbit_VisionTransformer(nbits=4, patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True)\n",
    "model2_params = sum(p.numel() for p in model2.parameters())\n",
    "print(f\"Total number of parameters in model2: {model2_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
